backbone:
  name: microsoft/deberta-v3-large
  config_path: null
  hidden_dropout: 0
  hidden_dropout_prob: 0
  attention_dropout: 0
  attention_probs_dropout_prob: 0
  freeze_embeddings: True
  freeze_n_layers: null

pool:
  name: ConcatenatePooling
  params:
    n_layers: 4

num_targets: 2

train:
  seed: 42
  model_name: CustomModel
  reinit_head_layer: True
  backbone_lr: 1e-5
  head_lr: 1e-3
  optimizer_config:
    name: AdamW
    weight_decay: 0.02
  scheduler:
    name: Cosine
    params:
      num_cycles: 0.5
      num_warmup_steps: 0
  criterion:
    name: MCRMSELoss
  enable_gradient_checkpointing: True
  batch_size: 4
  patience: 20
  max_epochs: 5
  min_epochs: 1
  val_check_intervals: 200
  gradient_clip_val: 1000
  gradient_accumulation_steps: 1

dataset:
  max_length: 1024
  include_prompt_text: False
  use_corrected_text: False
  target_cols:
    - content
    - wording

root_folder: data
group_name: db_l_conc_pool


